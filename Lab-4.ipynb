{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "438a6775",
   "metadata": {},
   "source": [
    "# Python Lab Activity 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e5301",
   "metadata": {},
   "source": [
    "# 100 Factorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37b44d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "num = 100\n",
    "fact = 1\n",
    "while num > 0:\n",
    "    fact = fact * num\n",
    "    num = num - 1\n",
    "\n",
    "print(fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd249d",
   "metadata": {},
   "source": [
    "# String is Immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eaca179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1828927604592\n",
      "1828927604592\n",
      "1828927604592\n",
      "1828927604592\n",
      "1828927604592\n",
      "1828927604592\n",
      "1828927604592\n",
      "1828927604592\n",
      "1828927604592\n",
      "1828927604592\n",
      "0123456789\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\n",
    "for i in range(10):\n",
    "    s += str(i)\n",
    "    print(id(s))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce4907",
   "metadata": {},
   "source": [
    "#  Alternative to save memory space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aa78d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1828927885744\n",
      "0123456789\n"
     ]
    }
   ],
   "source": [
    "char_list = []\n",
    "for i in range(10):\n",
    "    char_list.append(str(i))\n",
    "s = \"\".join(char_list)\n",
    "print(id(s))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7083dfcc",
   "metadata": {},
   "source": [
    "# Handling Character not in keyboard using UNICODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "648ee7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ω\n"
     ]
    }
   ],
   "source": [
    "print('\\u03A9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8cd61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef0fa7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😀 \n",
      " Ω\n"
     ]
    }
   ],
   "source": [
    "omega = unicodedata.lookup('GREEK CAPITAL LETTER OMEGA')\n",
    "smiley = unicodedata.lookup('GRINNING FACE')\n",
    "\n",
    "print(smiley,\"\\n\",omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae2583",
   "metadata": {},
   "source": [
    "# Encoding equivalent of Keyboard character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f36fd99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode value of the character is  \\u0021\n"
     ]
    }
   ],
   "source": [
    "character = input(\"Enter the character from keyboard: \")\n",
    "unicode = ord(character)\n",
    "unicode_seq = f'\\\\u{unicode:04x}'\n",
    "print(\"Unicode value of the character is \",unicode_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb48620",
   "metadata": {},
   "source": [
    "# Encoding Chinese character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "375d99a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode value of the character is  \\u4f60\n"
     ]
    }
   ],
   "source": [
    "character = '你'   # character = 'H'\n",
    "unicode = ord(character)\n",
    "unicode_seq = f'\\\\u{unicode:04x}'\n",
    "print(\"Unicode value of the character is \",unicode_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f6d935",
   "metadata": {},
   "source": [
    "# Encoding undocumented character (Tulu language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74505b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode value of the character is  \\u0ca4\n"
     ]
    }
   ],
   "source": [
    "character = 'ತ'   # character 'T'\n",
    "unicode = ord(character)\n",
    "unicode_seq = f'\\\\u{unicode:04x}'\n",
    "print(\"Unicode value of the character is \",unicode_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318374fb",
   "metadata": {},
   "source": [
    "# Natural Language Toolkit (NLTK) library to perform Part-of-Speech (POS) tagging on a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00a3fa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\monee\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\monee\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.8/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, nltk\n",
      "\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------- ----------------------------- 1/4 [regex]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ---------------------------------------- 4/4 [nltk]\n",
      "\n",
      "Successfully installed joblib-1.5.1 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a727ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package english_wordnet to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\english_wordnet.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package mock_corpus to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mock_corpus.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\monee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b766f43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('T', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('s', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('a', 'DT')]\n",
      "[('l', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('W', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('.', '.')]\n",
      "[('I', 'PRP')]\n",
      "[('t', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('M', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('S', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('r', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('s', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('.', '.')]\n",
      "[('S', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('v', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('d', 'NN')]\n",
      "[(',', ',')]\n",
      "[('t', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('.', '.')]\n",
      "[('W', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('s', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('p', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('.', '.')]\n",
      "[('N', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('m', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('l', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('v', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('i', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('b', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('.', '.')]\n",
      "[('I', 'PRP')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('d', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('u', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('c', 'NNS')]\n",
      "[('k', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[(',', ',')]\n",
      "[('b', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('l', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('o', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('t', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('k', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('s', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('v', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[(',', ',')]\n",
      "[('l', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('k', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('h', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('k', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texts = '''The dog was called Wellington. It belonged to Mrs Shears who was our friend. She lived on the opposite side of the road, two houses to the left.\n",
    "Wellington was a poodle. Not one of the small poodles that have hairstyles but a big poodle. It had curly black fur, but when you got close you could see that the skin underneath the fur was a very pale yellow, like chicken.\n",
    "'''\n",
    "for text in texts:\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "        print(tagged_words)   # This line prints the list of tuples, where each tuple contains a word and its corresponding POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c3be3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags for the first 5 sentences:\n",
      "Sentence 1: [('\\ufeffThe', 'NN'), ('Project', 'NNP'), ('Gutenberg', 'NNP'), ('eBook', 'NN'), ('of', 'IN'), ('Death', 'NNP'), ('of', 'IN'), ('a', 'DT'), ('hero', 'NN'), ('This', 'DT'), ('ebook', 'NN'), ('is', 'VBZ'), ('for', 'IN'), ('the', 'DT'), ('use', 'NN'), ('of', 'IN'), ('anyone', 'NN'), ('anywhere', 'RB'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('and', 'CC'), ('most', 'JJS'), ('other', 'JJ'), ('parts', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('world', 'NN'), ('at', 'IN'), ('no', 'DT'), ('cost', 'NN'), ('and', 'CC'), ('with', 'IN'), ('almost', 'RB'), ('no', 'DT'), ('restrictions', 'NNS'), ('whatsoever', 'RB'), ('.', '.')]\n",
      "Sentence 2: [('You', 'PRP'), ('may', 'MD'), ('copy', 'VB'), ('it', 'PRP'), (',', ','), ('give', 'VB'), ('it', 'PRP'), ('away', 'RB'), ('or', 'CC'), ('re-use', 'VB'), ('it', 'PRP'), ('under', 'IN'), ('the', 'DT'), ('terms', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Project', 'NNP'), ('Gutenberg', 'NNP'), ('License', 'NNP'), ('included', 'VBD'), ('with', 'IN'), ('this', 'DT'), ('ebook', 'NN'), ('or', 'CC'), ('online', 'NN'), ('at', 'IN'), ('www.gutenberg.org', 'NN'), ('.', '.')]\n",
      "Sentence 3: [('If', 'IN'), ('you', 'PRP'), ('are', 'VBP'), ('not', 'RB'), ('located', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), (',', ','), ('you', 'PRP'), ('will', 'MD'), ('have', 'VB'), ('to', 'TO'), ('check', 'VB'), ('the', 'DT'), ('laws', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('country', 'NN'), ('where', 'WRB'), ('you', 'PRP'), ('are', 'VBP'), ('located', 'VBN'), ('before', 'IN'), ('using', 'VBG'), ('this', 'DT'), ('eBook', 'NN'), ('.', '.')]\n",
      "Sentence 4: [('Title', 'NN'), (':', ':'), ('Death', 'NN'), ('of', 'IN'), ('a', 'DT'), ('hero', 'NN'), ('Author', 'NN'), (':', ':'), ('Richard', 'NNP'), ('Aldington', 'NNP'), ('Release', 'NNP'), ('date', 'NN'), (':', ':'), ('July', 'NNP'), ('27', 'CD'), (',', ','), ('2025', 'CD'), ('[', 'NNP'), ('eBook', 'VBD'), ('#', '#'), ('76571', 'CD'), (']', 'JJ'), ('Language', 'NN'), (':', ':'), ('English', 'JJ'), ('Original', 'NNP'), ('publication', 'NN'), (':', ':'), ('New', 'NNP'), ('York', 'NNP'), (':', ':'), ('Covici-Friede', 'NNP'), (',', ','), ('Inc', 'NNP'), (',', ','), ('1929', 'CD'), ('Credits', 'NNS'), (':', ':'), ('Sean/IB', 'NNP'), ('@', 'NNP'), ('DP', 'NNP'), ('*', 'NNP'), ('*', 'NNP'), ('*', 'NNP'), ('START', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('PROJECT', 'NNP'), ('GUTENBERG', 'NNP'), ('EBOOK', 'NNP'), ('DEATH', 'NNP'), ('OF', 'IN'), ('A', 'NNP'), ('HERO', 'NNP'), ('*', 'NNP'), ('*', 'NNP'), ('*', 'NNP'), ('DEATH', 'NNP'), ('OF', 'IN'), ('A', 'NNP'), ('HERO', 'NNP'), ('_A', 'NNP'), ('NOVEL', 'NNP'), ('BY', 'NNP'), ('RICHARD', 'NNP'), ('ALDINGTON_', 'NNP'), ('1929', 'CD'), ('COVICI', 'NNP'), ('·', 'NNP'), ('FRIEDE', 'NNP'), ('·', 'NNP'), ('INC', 'NNP'), ('·', 'NNP'), ('NEW', 'NNP'), ('YORK', 'NNP'), ('COPYRIGHT', 'NNP'), (',', ','), ('1929', 'CD'), (',', ','), ('BY', 'NNP'), ('COVICI-FRIEDE', 'NNP'), (',', ','), ('INC.', 'NNP'), ('Second', 'NNP'), ('printing', 'NN'), (',', ','), ('July', 'NNP'), (',', ','), ('1929', 'CD'), ('Third', 'NNP'), ('printing', 'NN'), (',', ','), ('July', 'NNP'), (',', ','), ('1929', 'CD'), ('Fourth', 'NNP'), ('printing', 'NN'), (',', ','), ('October', 'NNP'), (',', ','), ('1929', 'CD'), ('_Manufactured', 'VBD'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('of', 'IN'), ('America', 'NNP'), ('by', 'IN'), ('the', 'DT'), ('VAN', 'NNP'), ('REES', 'NNP'), ('PRESS', 'NNP'), (',', ','), ('New', 'NNP'), ('York_', 'NNP'), ('_', 'NNP'), ('“', 'NNP'), ('See', 'NNP'), ('how', 'WRB'), ('we', 'PRP'), ('trifle', 'VBP'), ('!', '.')]\n",
      "Sentence 5: [('but', 'CC'), ('one', 'CD'), ('can', 'MD'), ('’', 'VB'), ('t', 'JJ'), ('pass', 'NN'), ('one', 'CD'), ('’', 'NN'), ('s', 'NN'), ('youth', 'NN'), ('too', 'RB'), ('amusingly', 'RB'), (';', ':'), ('for', 'IN'), ('one', 'CD'), ('must', 'MD'), ('grow', 'VB'), ('old', 'JJ'), (',', ','), ('and', 'CC'), ('that', 'IN'), ('in', 'IN'), ('England', 'NNP'), (';', ':'), ('two', 'CD'), ('most', 'JJS'), ('serious', 'JJ'), ('circumstances', 'NNS'), (',', ','), ('either', 'DT'), ('of', 'IN'), ('which', 'WDT'), ('makes', 'VBZ'), ('people', 'NNS'), ('grey', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('twinkling', 'NN'), ('of', 'IN'), ('a', 'DT'), ('bed-staff', 'NN'), (';', ':'), ('for', 'IN'), ('know', 'NN'), ('you', 'PRP'), (',', ','), ('there', 'EX'), ('is', 'VBZ'), ('not', 'RB'), ('a', 'DT'), ('country', 'NN'), ('upon', 'IN'), ('earth', 'NN'), ('where', 'WRB'), ('there', 'EX'), ('are', 'VBP'), ('so', 'RB'), ('many', 'JJ'), ('old', 'JJ'), ('fools', 'NNS'), ('and', 'CC'), ('so', 'RB'), ('few', 'JJ'), ('young', 'JJ'), ('ones.', 'NNS'), ('”', 'JJ'), ('_', 'JJ'), ('HORACE', 'NNP'), ('WALPOLE', 'NNP'), ('TO', 'NNP'), ('_HALCOTT', 'NNP'), ('GLOVER_', 'NNP'), ('MY', 'NNP'), ('DEAR', 'NNP'), ('HAL', 'NNP'), (',', ','), ('Remembering', 'NNP'), ('George', 'NNP'), ('Moore', 'NNP'), ('’', 'NNP'), ('s', 'JJ'), ('denunciation', 'NN'), ('of', 'IN'), ('prefaces', 'NNS'), (',', ','), ('I', 'PRP'), ('felt', 'VBD'), ('that', 'IN'), ('what', 'WP'), ('I', 'PRP'), ('wanted', 'VBD'), ('to', 'TO'), ('say', 'VB'), ('here', 'RB'), ('could', 'MD'), ('be', 'VB'), ('best', 'JJS'), ('expressed', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('letter', 'NN'), ('to', 'TO'), ('you', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from urllib import request\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/76571/pg76571.txt\"\n",
    "\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf-8')\n",
    "\n",
    "# It's a good practice to remove the Project Gutenberg header/footer\n",
    "# This part is specific to the structure of Project Gutenberg texts\n",
    "start_of_book = \"*** START OF THE PROJECT GUTENBERG EBOOK CRIME AND PUNISHMENT ***\"\n",
    "end_of_book = \"*** END OF THE PROJECT GUTENBERG EBOOK CRIME AND PUNISHMENT ***\"\n",
    "\n",
    "# Find the start and end of the actual novel content\n",
    "start_index = raw.find(start_of_book)\n",
    "end_index = raw.find(end_of_book)\n",
    "\n",
    "# Extract the novel content\n",
    "if start_index != -1 and end_index != -1:\n",
    "    novel_text = raw[start_index + len(start_of_book):end_index].strip()\n",
    "elif start_index != -1:  # If only start marker is found\n",
    "    novel_text = raw[start_index + len(start_of_book):].strip()\n",
    "else:  # If no markers found, use the raw text (less accurate)\n",
    "    novel_text = raw\n",
    "\n",
    "# Tokenize the novel into sentences\n",
    "sentences = nltk.sent_tokenize(novel_text)\n",
    "\n",
    "# Process and print POS tags for each sentence\n",
    "# For brevity, let's process and print only the first few sentences\n",
    "print(\"POS Tags for the first 5 sentences:\")\n",
    "for i, sentence in enumerate(sentences[:5]): # Process only the first 5 sentences\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    print(f\"Sentence {i+1}: {tagged_words}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
